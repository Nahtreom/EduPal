#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ–‡æ¡£æ™ºèƒ½å¤„ç†ä¸èåˆç¨‹åº

åŠŸèƒ½ï¼š
1.  è¾“å…¥ä¸€ä¸ªåŒ…å«å¤šç¯‡å·²å¤„ç†è®ºæ–‡çš„ç›®å½• (æ¥è‡ª MinerU çš„ outputs_clean)ã€‚
2.  (Phase 1) å¯¹æ¯ç¯‡è®ºæ–‡è¿›è¡Œç‰©ç†åˆ†å‰²ï¼Œåˆ‡åˆ†ä¸º Abstract, Introduction, Methods, Experiments, Conclusion äº”ä¸ªéƒ¨åˆ†ã€‚
3.  (Phase 2) å¯¹åˆ†å‰²åçš„ç« èŠ‚è¿›è¡Œç‰©ç†æ‹¼æ¥ï¼Œå¹¶æ™ºèƒ½å¤„ç†å›¾ç‰‡è·¯å¾„ï¼Œä¸ºèåˆåšå‡†å¤‡ã€‚
4.  (Phase 3) è°ƒç”¨å¤§æ¨¡å‹å¯¹æ‹¼æ¥åçš„ç« èŠ‚è¿›è¡Œæ™ºèƒ½èåˆï¼Œè¾“å‡ºæœ€ç»ˆçš„å››ä¸ªæ ¸å¿ƒç« èŠ‚ã€‚

ä½¿ç”¨æ–¹æ³•: python multi_paper_document_processor.py <å¾…å¤„ç†è®ºæ–‡æ€»ç›®å½•> <æœ€ç»ˆè¾“å‡ºç›®å½•>
python multi_paper_document_processor.py ../MinerU/outputs_clean/muti_paper_inputs01 test_sections
"""

import os
import sys
import json
import re
import shutil
import glob
from pathlib import Path

# <--- CHANGE HERE: å¯¼å…¥ä¸¤ä¸ªå‡½æ•°
try:
    from api_call import process_text, process_text_with_images
except ImportError:
    print("é”™è¯¯: æœªæ‰¾åˆ° 'api_call.py' æ–‡ä»¶ã€‚è¯·ç¡®ä¿å®ƒä¸æœ¬è„šæœ¬åœ¨åŒä¸€ç›®å½•ä¸‹ã€‚")
    sys.exit(1)

# --- å…¨å±€é…ç½® ---
SECTIONS_TO_SPLIT = ["Abstract", "Introduction", "Methods", "Experiments", "Conclusion"]
SECTIONS_TO_FUSE = ["Introduction", "Methods", "Experiments", "Conclusion"]
FUSION_PROMPT_MAP = {
    'Introduction': 'Intro_Integration.txt',
    'Methods': 'Method_Integration.txt',
    'Experiments': 'Experiment_Integration.txt',
    'Conclusion': 'Conclusion_Integration.txt',
}


# --- 1. é…ç½®ä¸æ¨¡æ¿åŠ è½½ ---

def load_config():
    """ä»config.jsonåŠ è½½é…ç½®"""
    config_file = Path("config.json")
    if not config_file.exists():
        # <--- ä¸ç”¨base_urlï¼Œåœ¨ api_call.py ä¸­æ˜¯ç¡¬ç¼–ç 
        default_config = {
            "api_key": "your-api-key-here",
            "model": "gpt-4o"
        }
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(default_config, f, ensure_ascii=False, indent=2)
        print(f"å·²åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶ {config_file}ï¼Œè¯·ä¿®æ”¹å…¶ä¸­çš„ api_key")
        sys.exit(1)
    
    with open(config_file, 'r', encoding='utf-8') as f:
        config = json.load(f)
    if config.get("api_key") == "your-api-key-here":
        print("è¯·åœ¨ config.json ä¸­è®¾ç½®æ­£ç¡®çš„ api_key")
        sys.exit(1)
    return config

# ... (prepare_prompt_templates, load_prompt_template, read_file_content, parse_section_mapping å‡½æ•°ä¿æŒä¸å˜) ...
def prepare_prompt_templates():
    """æ£€æŸ¥å¹¶åˆ›å»ºæ‰€æœ‰éœ€è¦çš„promptæ¨¡æ¿æ–‡ä»¶"""
    prompt_dir = Path("prompt_template")
    prompt_dir.mkdir(exist_ok=True)

    # 1. åˆ†å‰²ç”¨çš„æ¨¡æ¿
    split_prompt_path = prompt_dir / "Central.txt"
    if not split_prompt_path.exists():
        split_prompt_content = """ä½ æ˜¯ä¸€åå­¦æœ¯å†™ä½œåŠ©ç†ï¼Œè¯·ä½ é˜…è¯»ä»¥ä¸‹å®Œæ•´æ–‡ç« å†…å®¹ï¼Œè¯†åˆ«å‡ºå…¶ä¸­å±äºä»¥ä¸‹äº”ä¸ªéƒ¨åˆ†çš„ç« èŠ‚æ ‡é¢˜ï¼š

1. Abstract
2. Introduction
3. æ–¹æ³• (Methods / Methodology)
4. å®éªŒ (Experiments / Experimental Setup / Evaluation)
5. ç»“è®º (Conclusion / Discussion / Summary)

è¯·ä½ ä»¥å¦‚ä¸‹æ ¼å¼è¾“å‡ºæ¯ä¸ªéƒ¨åˆ†å¯¹åº”çš„ç« èŠ‚æ ‡é¢˜ï¼ˆåªç»™å‡ºå¤§æ ‡é¢˜å³å¯ï¼‰ï¼š

Abstract: <å¯¹åº”ç« èŠ‚æ ‡é¢˜>
Introduction: <å¯¹åº”ç« èŠ‚æ ‡é¢˜>
Methods: <å¯¹åº”ç« èŠ‚æ ‡é¢˜>
Experiments: <å¯¹åº”ç« èŠ‚æ ‡é¢˜>
Conclusion: <å¯¹åº”ç« èŠ‚æ ‡é¢˜>

ä»¥ä¸‹æ˜¯æ–‡ç« å†…å®¹ï¼š"""
        split_prompt_path.write_text(split_prompt_content, encoding='utf-8')
        print(f"å·²åˆ›å»ºåˆ†å‰²promptæ¨¡æ¿: {split_prompt_path}")

    # 2. èåˆç”¨çš„æ¨¡æ¿
    fusion_prompts = {
        "Intro_Integration.txt": "èåˆä»¥ä¸‹å¤šç¯‡è®ºæ–‡çš„å¼•è¨€(Introduction)éƒ¨åˆ†...",
        "Method_Integration.txt": "èåˆä»¥ä¸‹å¤šç¯‡è®ºæ–‡çš„æ–¹æ³•(Method)éƒ¨åˆ†...",
        "Experiment_Integration.txt": "èåˆä»¥ä¸‹å¤šç¯‡è®ºæ–‡çš„å®éªŒ(Experiment)éƒ¨åˆ†...",
        "Conclusion_Integration.txt": "èåˆä»¥ä¸‹å¤šç¯‡è®ºæ–‡çš„ç»“è®º(Conclusion)éƒ¨åˆ†...",
    }
    
    base_fusion_prompt = """ä½ æ˜¯ä¸€ä½é¡¶çº§çš„å­¦æœ¯ç ”ç©¶å‘˜ï¼Œæ“…é•¿æ•´åˆå’Œæç‚¼å¤šç¯‡è®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³ã€‚
ä½ çš„ä»»åŠ¡æ˜¯é˜…è¯»ä¸‹é¢æä¾›çš„ã€æ¥è‡ªå¤šç¯‡è®ºæ–‡çš„åŒä¸€ç« èŠ‚å†…å®¹ï¼Œä»¥åŠå®ƒä»¬çš„æ‘˜è¦ä½œä¸ºä¸Šä¸‹æ–‡ã€‚è¯·å°†è¿™äº›å†…å®¹èåˆã€é‡å†™å¹¶æ€»ç»“æˆä¸€ä¸ªå…¨æ–°çš„ã€è¿è´¯æµç•…çš„ç« èŠ‚ã€‚

è¦æ±‚ï¼š
1.  **é€»è¾‘æ¸…æ™°**ï¼šç¡®ä¿æœ€ç»ˆçš„æ–‡æœ¬ç»“æ„åˆç†ï¼Œé€»è¾‘é€’è¿›ã€‚
2.  **ä¿ç•™æ ¸å¿ƒ**ï¼šä¿ç•™æ¯ç¯‡è®ºæ–‡æœ€å…³é”®çš„è§‚ç‚¹ã€æ–¹æ³•æˆ–å‘ç°ã€‚
3.  **è¯†åˆ«å¼‚åŒ**ï¼šåœ¨èåˆæ—¶ï¼Œå¯ä»¥ subtly æŒ‡å‡ºä¸åŒè®ºæ–‡é—´çš„å…±é€šä¹‹å¤„æˆ–ç‹¬ç‰¹æ–¹æ³•ã€‚
4.  **å¤„ç†å›¾ç‰‡**ï¼šæ–‡ä¸­çš„å›¾ç‰‡å¼•ç”¨æ ¼å¼ä¸º `![å›¾ç‰‡æè¿°](è·¯å¾„/å›¾ç‰‡æ–‡ä»¶å)`ã€‚åœ¨ä½ çš„è¾“å‡ºä¸­ï¼Œå¿…é¡»å®Œæ•´åœ°ä¿ç•™è¿™äº›å›¾ç‰‡å¼•ç”¨ï¼Œå¹¶ç¡®ä¿å®ƒä»¬åœ¨è¡Œæ–‡é€»è¾‘ä¸­è¢«æ°å½“åœ°æåŠã€‚ä¸è¦çœç•¥ä»»ä½•å›¾ç‰‡ï¼
5.  **æä¾›ä¸Šä¸‹æ–‡**ï¼šä¸‹é¢æä¾›äº†æ¯ç¯‡è®ºæ–‡çš„æ‘˜è¦ï¼Œå¸®åŠ©ä½ ç†è§£å„ç¯‡è®ºæ–‡çš„æ•´ä½“è´¡çŒ®ã€‚

---
[è®ºæ–‡æ‘˜è¦ä¸Šä¸‹æ–‡]
{abstracts_content}
---

---
[å¾…èåˆçš„ç« èŠ‚å†…å®¹]
{section_content}
---

è¯·ç°åœ¨å¼€å§‹ä½ çš„èåˆå†™ä½œï¼š"""

    for filename, description in fusion_prompts.items():
        prompt_path = prompt_dir / filename
        if not prompt_path.exists():
            prompt_path.write_text(base_fusion_prompt, encoding='utf-8')
            print(f"å·²åˆ›å»ºèåˆpromptæ¨¡æ¿: {prompt_path}")

def load_prompt_template(filename: str) -> str:
    template_file = Path("prompt_template") / filename
    if not template_file.exists():
        raise FileNotFoundError(f"Promptæ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {template_file}")
    return template_file.read_text(encoding='utf-8')

def read_file_content(file_path: Path) -> str:
    return file_path.read_text(encoding='utf-8')

def parse_section_mapping(model_output: str) -> dict:
    sections = {}
    for line in model_output.strip().split('\n'):
        if ':' in line:
            parts = line.split(':', 1)
            if len(parts) == 2 and parts[0].strip() in SECTIONS_TO_SPLIT:
                sections[parts[0].strip()] = parts[1].strip()
    return sections

# v-- ä»£ç ç§»æ¤å¼€å§‹ --v
# æ³¨é‡Šï¼šä»¥ä¸‹ä¸‰ä¸ªå‡½æ•° (extract_section_number, is_subsection, find_section_content) 
# ä»èƒ½å¤Ÿæ­£ç¡®å¤„ç†å­ç« èŠ‚çš„ "document_processor.py" è„šæœ¬ä¸­å®Œæ•´ç§»æ¤è€Œæ¥ï¼Œ
# ä»¥æ›¿æ¢åŸæœ‰è„šæœ¬ä¸­åŠŸèƒ½ä¸å®Œå–„çš„ find_section_content å‡½æ•°ã€‚
def extract_section_number(title_line: str) -> str:
    """ä»æ ‡é¢˜è¡Œä¸­æå–ç« èŠ‚ç¼–å·"""
    # ç§»é™¤å¼€å¤´çš„#å·å’Œç©ºæ ¼
    title_text = re.sub(r'^#+\s*', '', title_line).strip()
    # æå–å¼€å¤´çš„æ•°å­—éƒ¨åˆ†ï¼ˆå¯èƒ½åŒ…å«å°æ•°ç‚¹ï¼‰
    match = re.match(r'^(\d+(?:\.\d+)*)', title_text)
    return match.group(1) if match else ""

def is_subsection(parent_num: str, current_num: str) -> bool:
    """åˆ¤æ–­current_numæ˜¯å¦æ˜¯parent_numçš„å­ç« èŠ‚"""
    if not parent_num or not current_num:
        return False
    
    # å¦‚æœcurrent_numä»¥parent_numå¼€å¤´ä¸”åé¢è·Ÿç€å°æ•°ç‚¹ï¼Œåˆ™æ˜¯å­ç« èŠ‚
    return current_num.startswith(parent_num + ".")

def find_section_content(markdown_content: str, section_identifier: str) -> str:
    """åœ¨markdownä¸­æ‰¾åˆ°æŒ‡å®šç« èŠ‚çš„å†…å®¹ï¼ˆç§»æ¤è‡ªå•ç¯‡å¤„ç†è„šæœ¬çš„å®Œå–„ç‰ˆæœ¬ï¼‰"""
    lines = markdown_content.split('\n')
    content_lines = []
    in_section = False
    found_line = None
    
    # ä¸å†æ‰“å°è¯¦ç»†æœç´¢è¿‡ç¨‹ï¼Œä»¥ä¿æŒå¤šè®ºæ–‡å¤„ç†æµç¨‹çš„è¾“å‡ºæ•´æ´
    # print(f"  æ­£åœ¨æœç´¢ç« èŠ‚: '{section_identifier}'")
    
    # åˆ›å»ºå¤šç§å¯èƒ½çš„æ ‡é¢˜æ¨¡å¼
    pattern1 = rf'^#+\s*{re.escape(section_identifier)}\s*$'
    pattern2 = rf'^#+\s*.*{re.escape(section_identifier)}.*$'
    section_text_only = re.sub(r'^\d+(\.\d+)*\s*', '', section_identifier).strip()
    pattern3 = rf'^#+\s*[\d.]*\s*{re.escape(section_text_only)}\s*$' if section_text_only else None
    
    patterns = [p for p in [pattern1, pattern2, pattern3] if p]
    
    # æœç´¢åŒ¹é…çš„æ ‡é¢˜è¡Œ
    for i, line in enumerate(lines):
        line_stripped = line.strip()
        if line_stripped.startswith('#'):
            for pattern in patterns:
                if re.match(pattern, line_stripped, re.IGNORECASE):
                    found_line = i
                    in_section = True
                    content_lines.append(line)
                    break
            if found_line is not None:
                break
    
    if found_line is None:
        print(f"  [è­¦å‘Š] æœªèƒ½æ‰¾åˆ°ç« èŠ‚æ ‡é¢˜: '{section_identifier}'")
        return ""
    
    # è·å–èµ·å§‹æ ‡é¢˜çš„çº§åˆ«å’Œç« èŠ‚å·
    start_title_level_match = re.match(r'^(#+)', lines[found_line])
    if not start_title_level_match:
        # å¦‚æœåŒ¹é…çš„è¡Œä¸æ˜¯æœ‰æ•ˆçš„æ ‡é¢˜è¡Œï¼ˆè™½ç„¶ç†è®ºä¸Šä¸ä¼šå‘ç”Ÿï¼‰ï¼Œåˆ™è¿”å›ç©º
        return ""
    start_title_level = len(start_title_level_match.group(1))
    start_section_number = extract_section_number(lines[found_line])
    
    # ä»æ‰¾åˆ°çš„è¡Œå¼€å§‹ï¼Œæ”¶é›†å†…å®¹ç›´åˆ°ä¸‹ä¸€ä¸ªéå­ç« èŠ‚æ ‡é¢˜
    for i in range(found_line + 1, len(lines)):
        line = lines[i]
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜è¡Œ
        if re.match(r'^#+\s*\S', line):
            current_level_match = re.match(r'^(#+)', line)
            if not current_level_match: continue
            
            current_level = len(current_level_match.group(1))
            current_section_number = extract_section_number(line)
            
            # åˆ¤æ–­æ˜¯å¦åº”è¯¥åœæ­¢æ”¶é›†
            should_stop = False
            if current_level <= start_title_level:
                # é‡åˆ°åŒçº§æˆ–æ›´é«˜çº§æ ‡é¢˜
                if start_section_number and current_section_number and is_subsection(start_section_number, current_section_number):
                    # æ˜¯å­ç« èŠ‚çš„ç‰¹æ®Šæƒ…å†µï¼ˆä¾‹å¦‚ 2. vs 2.1ï¼‰ï¼Œè™½ç„¶çº§åˆ«ç›¸åŒä½†åº”ç»§ç»­
                    pass
                else:
                    # ä¸æ˜¯å­ç« èŠ‚ï¼Œåœæ­¢
                    should_stop = True

            if should_stop:
                break
        
        content_lines.append(line)
    
    result = '\n'.join(content_lines)
    return result.strip()
# ^-- ä»£ç ç§»æ¤ç»“æŸ --^


# --- Phase 1: ç‰©ç†åˆ†å‰² ---
def split_single_paper(paper_dir: Path, temp_split_dir: Path, config: dict):
    paper_name = paper_dir.name
    print(f"\n--- [Phase 1] æ­£åœ¨åˆ†å‰²è®ºæ–‡: {paper_name} ---")
    
    md_files = list(paper_dir.glob("*.md"))
    if not md_files:
        print(f"  [é”™è¯¯] åœ¨ {paper_dir} ä¸­æœªæ‰¾åˆ°.mdæ–‡ä»¶ï¼Œè·³è¿‡ã€‚")
        return False

    markdown_path = md_files[0]
    paper_content = read_file_content(markdown_path)
    
    split_prompt_template = load_prompt_template("Central.txt")
    full_prompt = f"{split_prompt_template}\n\n{paper_content}"
    
    print("  æ­£åœ¨è°ƒç”¨LLMåˆ†æç« èŠ‚ç»“æ„...")
    sections_map = {}
    try:
        mapping_result = process_text(
            full_prompt, 
            config["api_key"], 
            config["model"]
        )
        sections_map = parse_section_mapping(mapping_result)
        print("  LLMç« èŠ‚æ˜ å°„è§£æå®Œæˆ:")
        for sec, title in sections_map.items():
            print(f"    - {sec}: {title}")
    except Exception as e:
        print(f"  [é”™è¯¯] LLMè°ƒç”¨å¤±è´¥: {e}")
        return False
    
    # ++++++++++++++++++++++++++++++++
    # +++    å¢åŠ Abstractï¼šå¯å‘å¼åå¤‡è§„åˆ™   +++
    # ++++++++++++++++++++++++++++++++
    if 'Abstract' not in sections_map:
        print("  [ä¿¡æ¯] LLMæœªè¿”å›Abstractï¼Œå¯åŠ¨å¯å‘å¼è§„åˆ™è¿›è¡ŒæŸ¥æ‰¾...")
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾ '# ABSTRACT' (ä¸åŒºåˆ†å¤§å°å†™, ç‹¬å ä¸€è¡Œ)
        abstract_match = re.search(r'^#\s+ABSTRACT\s*$', paper_content, re.IGNORECASE | re.MULTILINE)
        
        if abstract_match:
            # ä»åŒ¹é…åˆ°çš„æ–‡æœ¬ä¸­æå–å‡ºå¹²å‡€çš„æ ‡é¢˜ (ä¾‹å¦‚ "ABSTRACT")
            found_title = abstract_match.group(0).strip('#').strip()
            sections_map['Abstract'] = found_title
            print(f"  [æˆåŠŸ] å¯å‘å¼è§„åˆ™æ‰¾åˆ°Abstractï¼Œæ ‡é¢˜ä¸º: '{found_title}'")
        else:
            print("  [è­¦å‘Š] å¯å‘å¼è§„åˆ™ä¹Ÿæœªèƒ½æ‰¾åˆ°Abstractã€‚")
    # ++++++++++++++++++++++++++++++++
    # +++        ä¿®æ”¹ç»“æŸ           +++
    # ++++++++++++++++++++++++++++++++

    # æ£€æŸ¥æ‰€æœ‰å¿…éœ€ç« èŠ‚æ˜¯å¦éƒ½è¢«è¯†åˆ«
    missing_sections = [sec for sec in SECTIONS_TO_SPLIT if sec not in sections_map]
    if missing_sections:
        print(f"  [è­¦å‘Š] LLMæˆ–å¯å‘å¼è§„åˆ™æœªèƒ½è¯†åˆ«å‡ºä»¥ä¸‹å¿…éœ€ç« èŠ‚: {', '.join(missing_sections)}ã€‚åˆ†å‰²å¯èƒ½ä¸å®Œæ•´ã€‚")
    else:
        print("  [æˆåŠŸ] å·²è¯†åˆ«æ‰€æœ‰å¿…éœ€ç« èŠ‚ã€‚")


    paper_output_dir = temp_split_dir / paper_name
    paper_output_dir.mkdir(exist_ok=True)
    
    for section_name in SECTIONS_TO_SPLIT:
        if section_name in sections_map:
            section_title = sections_map[section_name]
            content = find_section_content(paper_content, section_title)
            if content:
                out_path = paper_output_dir / f"{paper_name}_{section_name}.md"
                out_path.write_text(content, encoding='utf-8')
                print(f"  å·²ä¿å­˜: {out_path.name}")
            else:
                # å¦‚æœæ‰¾åˆ°äº†æ ‡é¢˜ä½†æ²¡æ‰¾åˆ°å†…å®¹ï¼Œä¹Ÿåˆ›å»ºç©ºæ–‡ä»¶å¹¶è­¦å‘Š
                print(f"  [è­¦å‘Š] æ‰¾åˆ°äº†æ ‡é¢˜ '{section_title}' ä½†æœªèƒ½æå–å†…å®¹ã€‚")
                out_path = paper_output_dir / f"{paper_name}_{section_name}.md"
                out_path.write_text(f"# {section_name}\n\n(æœªèƒ½æå–åˆ°å†…å®¹)", encoding='utf-8')
    return True

# ... (åç»­åŸä»£ç concatenate_sections, fuse_all_sections, main ç­‰ä¿æŒä¸å˜) ...
# --- Phase 2: ç‰©ç†æ‹¼æ¥ ---
def concatenate_sections(paper_dirs: list, temp_split_dir: Path, temp_concat_dir: Path):
    paper_names = [p.name for p in paper_dirs]
    print(f"\n--- [Phase 2] æ­£åœ¨æ‹¼æ¥ {len(paper_names)} ç¯‡è®ºæ–‡çš„ç« èŠ‚ ---")
    
    combined_images_dir_name = "combined_images"
    
    for section_name in SECTIONS_TO_FUSE:
        print(f"  æ­£åœ¨å¤„ç†ç« èŠ‚: {section_name}")
        
        all_content = []
        for paper_dir in paper_dirs:
            paper_name = paper_dir.name
            section_file = temp_split_dir / paper_name / f"{paper_name}_{section_name}.md"
            
            if section_file.exists():
                content = section_file.read_text(encoding='utf-8')
                
                # å°† ![...](images/...) æ›¿æ¢ä¸º ![...](combined_images/PaperName_images/...)
                rewritten_content = re.sub(
                    r"!\[(.*?)\]\((images/.*?)\)",
                    lambda m: f"![{m.group(1)}]({combined_images_dir_name}/{paper_name}_{m.group(2)})",
                    content
                )
                
                header = f"\n\n---\n\n# åŸæ–‡: {paper_name}\n\n"
                all_content.append(header + rewritten_content)

                original_images_dir = paper_dir / "images"
                if original_images_dir.is_dir():
                    target_images_dir = temp_concat_dir / combined_images_dir_name / f"{paper_name}_images"
                    if not target_images_dir.exists():
                        shutil.copytree(original_images_dir, target_images_dir)
                        print(f"    - å·²æ‹·è´å›¾ç‰‡: {original_images_dir} -> {target_images_dir}")
            
        if all_content:
            concat_filename = f"{'+'.join(paper_names)}_{section_name}_CONCAT.md"
            concat_filepath = temp_concat_dir / concat_filename
            concat_filepath.write_text('\n'.join(all_content), encoding='utf-8')
            print(f"  å·²æ‹¼æ¥ä¿å­˜è‡³: {concat_filepath.name}")

# --- Phase 3: æ™ºèƒ½èåˆ ---
def fuse_all_sections(paper_names: list, temp_concat_dir: Path, final_output_dir: Path, config: dict):
    print(f"\n--- [Phase 3] æ­£åœ¨æ™ºèƒ½èåˆç« èŠ‚ ---")
    
    temp_split_dir = temp_concat_dir.parent / "temp_split_sections"
    abstracts_content = []
    for paper_name in paper_names:
        abstract_file = temp_split_dir / paper_name / f"{paper_name}_Abstract.md"
        if abstract_file.exists():
            abstracts_content.append(f"æ‘˜è¦ ({paper_name}):\n{abstract_file.read_text(encoding='utf-8')}\n")
    
    full_abstracts_context = "---\n".join(abstracts_content)

    for section_name in SECTIONS_TO_FUSE:
        print(f"  æ­£åœ¨èåˆç« èŠ‚: {section_name}")
        
        concat_filename = f"{'+'.join(paper_names)}_{section_name}_CONCAT.md"
        concat_filepath = temp_concat_dir / concat_filename
        
        if not concat_filepath.exists():
            print(f"    [è­¦å‘Š] æ‰¾ä¸åˆ°æ‹¼æ¥æ–‡ä»¶ {concat_filename}ï¼Œè·³è¿‡èåˆã€‚")
            continue
            
        section_content = concat_filepath.read_text(encoding='utf-8')
        
        prompt_filename = FUSION_PROMPT_MAP.get(section_name)
        if not prompt_filename:
            print(f"    [è­¦å‘Š] æ‰¾ä¸åˆ° {section_name} çš„èåˆpromptï¼Œè·³è¿‡ã€‚")
            continue
            
        fusion_prompt_template = load_prompt_template(prompt_filename)
        final_prompt = fusion_prompt_template.replace("{abstracts_content}", full_abstracts_context)
        final_prompt = final_prompt.replace("{section_content}", section_content)
        
        print(f"    è°ƒç”¨ {config['model']} (å¤šæ¨¡æ€) è¿›è¡Œèåˆ...")
        try:
            fused_content = process_text_with_images(
                text=final_prompt,
                api_key=config["api_key"],
                model=config["model"],
                base_path=str(temp_concat_dir) 
            )
            
            final_filename = f"{'+'.join(paper_names)}_{section_name}.md"
            final_filepath = final_output_dir / final_filename
            final_filepath.write_text(fused_content, encoding='utf-8')
            print(f"  ğŸ‰ èåˆæˆåŠŸ! å·²ä¿å­˜è‡³: {final_filepath}")

        except Exception as e:
            print(f"    [é”™è¯¯] èåˆAPIè°ƒç”¨å¤±è´¥: {e}")
            
    combined_images_dir = temp_concat_dir / "combined_images"
    if combined_images_dir.exists():
        final_images_dir = final_output_dir / combined_images_dir.name
        if final_images_dir.exists():
            shutil.rmtree(final_images_dir)
        shutil.copytree(combined_images_dir, final_images_dir)
        print(f"\nå·²å°†æ‰€æœ‰å›¾ç‰‡æ‹·è´è‡³: {final_images_dir}")

# --- ä¸»å‡½æ•° ---
def main():
    if len(sys.argv) != 3:
        print("ä½¿ç”¨æ–¹æ³•: python multi_paper_document_processor.py <å¾…å¤„ç†è®ºæ–‡æ€»ç›®å½•> <æœ€ç»ˆè¾“å‡ºç›®å½•>")
        print("ç¤ºä¾‹: python multi_paper_document_processor.py ../MinerU/outputs_clean/muti_paper_inputs01 ./fused_sections")
        sys.exit(1)
        
    input_dir = Path(sys.argv[1])
    output_dir = Path(sys.argv[2])
    
    if not input_dir.is_dir():
        print(f"é”™è¯¯: è¾“å…¥ç›®å½• '{input_dir}' ä¸å­˜åœ¨æˆ–ä¸æ˜¯ä¸€ä¸ªç›®å½•ã€‚")
        sys.exit(1)
        
    paper_dirs = [d for d in input_dir.iterdir() if d.is_dir()]
    if not paper_dirs:
        print(f"é”™è¯¯: åœ¨ '{input_dir}' ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è®ºæ–‡å­ç›®å½•ã€‚")
        sys.exit(1)
        
    print(f"æ£€æµ‹åˆ° {len(paper_dirs)} ç¯‡è®ºæ–‡å¾…å¤„ç†: {[p.name for p in paper_dirs]}")

    try:
        config = load_config()
        prepare_prompt_templates()

        temp_dir = output_dir / "temp_processing"
        if temp_dir.exists():
            shutil.rmtree(temp_dir)
        temp_split_dir = temp_dir / "temp_split_sections"
        temp_concat_dir = temp_dir / "temp_concatenated"
        temp_split_dir.mkdir(parents=True, exist_ok=True)
        temp_concat_dir.mkdir(parents=True, exist_ok=True)
        
        successful_splits = []
        for paper_dir in paper_dirs:
            if split_single_paper(paper_dir, temp_split_dir, config):
                successful_splits.append(paper_dir)
        
        if not successful_splits:
            print("\næ‰€æœ‰è®ºæ–‡åˆ†å‰²å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
            sys.exit(1)

        concatenate_sections(successful_splits, temp_split_dir, temp_concat_dir)

        output_dir.mkdir(exist_ok=True)
        fuse_all_sections(
            [p.name for p in successful_splits], 
            temp_concat_dir, 
            output_dir, 
            config
        )

        print(f"\nå¤„ç†å®Œæˆï¼ä¸´æ—¶æ–‡ä»¶ä¿ç•™åœ¨ {temp_dir} ä¾›è°ƒè¯•ã€‚å¯æ‰‹åŠ¨åˆ é™¤ã€‚")
        print(f"æœ€ç»ˆèåˆç»“æœå·²ä¿å­˜åœ¨: {output_dir}")

    except Exception as e:
        print(f"\nç¨‹åºå‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()