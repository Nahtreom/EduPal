from exa_py import Exa
import re

# æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ Exa API å¯†é’¥
EXA_API_KEY = '211ed197-9199-401d-9897-a8408526f8b8'
exa = Exa(EXA_API_KEY)

def main():
    """ä¸»å‡½æ•°ï¼Œæä¾›é€‰æ‹©èœå•"""
    print("="*50)
    print("          æ¬¢è¿ä½¿ç”¨æœç´¢å·¥å…·")
    print("="*50)
    print("è¯·é€‰æ‹©æœç´¢ç±»å‹ï¼š")
    print("(1) æŸ¥æ‰¾ç½‘é¡µ")
    print("(2) æŸ¥æ‰¾è®ºæ–‡")
    print("-"*50)
    
    while True:
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2): ").strip()
        if choice in ['1', '2']:
            break
        print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1 æˆ– 2")
    
    # è·å–æŸ¥è¯¢å…³é”®è¯
    search_keyword = input("è¯·è¾“å…¥è¦æœç´¢çš„å…³é”®è¯: ")
    
    if choice == '1':
        search_web(search_keyword)
    else:
        search_papers(search_keyword)

def search_web(search_keyword):
    """æœç´¢ç½‘é¡µ"""
    print(f"\næ­£åœ¨æœç´¢ç½‘é¡µ: {search_keyword}")
    print("="*80)
    
    # è°ƒç”¨ search_and_contents æ–¹æ³•è¿›è¡Œæ£€ç´¢
    result = exa.search_and_contents(
        search_keyword,
        text=True
    )
    
    print(f"æœç´¢ç»“æœï¼š{search_keyword}ç›¸å…³ç½‘é¡µ (å…±æ‰¾åˆ°: {len(result.results)}ä¸ª)")
    print("="*80)
    
    if not result.results:
        print("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç½‘é¡µã€‚")
    else:
        for idx, item in enumerate(result.results):
            print(f"\nğŸŒ ç½‘é¡µ {idx+1}")
            print("-" * 60)
            
            # æå–å¹¶æ‰“å°æ ‡é¢˜
            title = item.title if item.title else "[No Title]"
            print(f"æ ‡é¢˜: {title}")
            
            # æå–å¹¶æ‰“å°URL
            url = item.url if item.url else "[No URL]"
            print(f"é“¾æ¥: {url}")
            
            # æå–å¹¶æ‰“å°ç½‘é¡µå†…å®¹æ‘˜è¦ï¼ˆæ¸…ç†åæ˜¾ç¤ºæ›´å¤šå†…å®¹ï¼‰
            content = clean_web_content(item.text) if item.text else "[No Content]"
            if len(content) > 1000:
                content = content[:1000] + "..."
            print(f"å†…å®¹: {content}")
            
            print("-" * 60)

def search_papers(search_keyword):
    """æœç´¢è®ºæ–‡"""
    query = f"site:arxiv.org/abs {search_keyword}"
    print(f"\næ­£åœ¨æœç´¢è®ºæ–‡: {search_keyword}")
    print(f"æœç´¢æŸ¥è¯¢: {query}")
    print("="*80)
    
    # è°ƒç”¨ search_and_contents æ–¹æ³•è¿›è¡Œæ£€ç´¢ï¼Œå¹¶è·å–æ­£æ–‡å†…å®¹
    result = exa.search_and_contents(
        query,
        text=True
    )
    
    # ç­›é€‰ç»“æœ
    print(f"åŸå§‹æœç´¢ç»“æœ: {len(result.results)}ç¯‡")
    valid_results = []
    filtered_results = []
    
    for item in result.results:
        if is_valid_arxiv_url(item.url):
            valid_results.append(item)
        else:
            filtered_results.append(item)
    
    print(f"ç­›é€‰åç»“æœ: {len(valid_results)}ç¯‡ (è¿‡æ»¤æ‰: {len(filtered_results)}ç¯‡)")
    
    # æ˜¾ç¤ºè¢«è¿‡æ»¤æ‰çš„URLï¼Œä¾¿äºè°ƒè¯•
    if filtered_results:
        print(f"\nè¢«è¿‡æ»¤çš„URLç¤ºä¾‹:")
        for i, item in enumerate(filtered_results[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"  {i+1}. {item.url}")
    
    print(f"\næœç´¢ç»“æœï¼š{search_keyword}ç›¸å…³è®ºæ–‡")
    print("="*80)
    
    if not valid_results:
        print("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„arxivè®ºæ–‡ã€‚")
    else:
        for idx, item in enumerate(valid_results):
            print(f"\nğŸ“„ è®ºæ–‡ {idx+1}")
            print("-" * 60)
            
            # æå–å¹¶æ‰“å°æ ‡é¢˜
            title = item.title if item.title else "[No Title]"
            print(f"æ ‡é¢˜: {title}")
            
            # æå–å¹¶æ‰“å°URL
            url = item.url if item.url else "[No URL]"
            print(f"é“¾æ¥: {url}")
            
            # æå–å¹¶æ‰“å°æ‘˜è¦
            abstract = extract_abstract(item.text)
            print(f"æ‘˜è¦: {abstract}")
            
            print("-" * 60)

def extract_abstract(text):
    """ä»æ–‡æœ¬ä¸­æå–æ‘˜è¦ - æ”¹è¿›ç‰ˆæœ¬"""
    if not text:
        return "[No Abstract Found]"
    
    # é¦–å…ˆæ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤å¸¸è§çš„æ— å…³å†…å®¹
    # ç§»é™¤ä¸‹è½½é“¾æ¥ã€ä½œè€…ä¿¡æ¯ç­‰
    text = re.sub(r'Download PDF.*?(?=\n|$)', '', text, flags=re.IGNORECASE)
    text = re.sub(r'Authors?:\s*\[.*?\].*?(?=\n|$)', '', text, flags=re.IGNORECASE)
    text = re.sub(r'Submitted on.*?(?=\n|$)', '', text, flags=re.IGNORECASE)
    text = re.sub(r'Comments:.*?(?=\n|$)', '', text, flags=re.IGNORECASE)
    text = re.sub(r'Subjects:.*?(?=\n|$)', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\[.*?\]\(https?://.*?\)', '', text)  # ç§»é™¤markdowné“¾æ¥
    
    # é’ˆå¯¹arxivè®ºæ–‡æ ¼å¼çš„æ”¹è¿›æ¨¡å¼ - ç›´æ¥æ’é™¤"Abstract:"
    abstract_patterns = [
        # åŒ¹é…æ ‡å‡†çš„Abstractéƒ¨åˆ†ï¼Œä½†æ’é™¤"Abstract:"æœ¬èº«
        r'(?i)(?:^|\n)\s*abstract\s*:?\s*(.*?)(?=\n\s*(?:keywords?|introduction|1\.|references|related work|background|\n\s*[A-Z][a-z]+\s*:|\n\s*\d+\s+[A-Z])|$)',
        # åŒ¹é…markdownæ ¼å¼çš„Abstractæ ‡é¢˜ï¼Œæ’é™¤æ ‡é¢˜æœ¬èº«
        r'(?i)(?:^|\n)\s*#{1,6}\s*abstract\s*:?\s*(.*?)(?=\n\s*#{1,6}|\n\s*\d+\s+[A-Z]|$)',
        # æ›´å®½æ³›çš„æ¨¡å¼ï¼ŒåŒ¹é…Abstractåé¢çš„å†…å®¹
        r'(?i)abstract\s*:?\s*(.*?)(?=\n\s*(?:keywords?|introduction|1\.|references)|$)',
    ]
    
    for pattern in abstract_patterns:
        match = re.search(pattern, text, re.DOTALL | re.MULTILINE)
        if match:
            abstract = match.group(1).strip()
            
            # æ¸…ç†æå–çš„æ‘˜è¦
            abstract = clean_abstract(abstract)
            
            # æ£€æŸ¥æ‘˜è¦è´¨é‡
            if is_valid_abstract(abstract):
                return abstract
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†çš„Abstractï¼Œå°è¯•æ›´æ™ºèƒ½çš„æå–
    # å¯»æ‰¾å¯èƒ½çš„æ‘˜è¦æ®µè½ï¼ˆé€šå¸¸åœ¨å¼€å¤´å‡ æ®µï¼‰
    paragraphs = text.split('\n\n')
    for i, paragraph in enumerate(paragraphs[:5]):  # åªæ£€æŸ¥å‰5æ®µ
        cleaned_para = clean_abstract(paragraph)
        if is_valid_abstract(cleaned_para) and len(cleaned_para) > 100:
            return cleaned_para
    
    return "[Abstract Not Found]"

def clean_web_content(content):
    """æ¸…ç†ç½‘é¡µå†…å®¹ï¼Œç§»é™¤æ— å…³å…ƒç´ """
    if not content:
        return ""
    
    # ç§»é™¤å›¾ç‰‡ç›¸å…³å†…å®¹
    content = re.sub(r'!\[.*?\]\(.*?\)', '', content)  # markdownå›¾ç‰‡
    content = re.sub(r'<img[^>]*>', '', content, flags=re.IGNORECASE)  # HTMLå›¾ç‰‡æ ‡ç­¾
    
    # ç§»é™¤é“¾æ¥
    content = re.sub(r'\[.*?\]\(https?://[^\)]+\)', '', content)  # markdowné“¾æ¥
    content = re.sub(r'https?://[^\s]+', '', content)  # ç›´æ¥çš„URL
    
    # ç§»é™¤HTMLæ ‡ç­¾
    content = re.sub(r'<[^>]+>', '', content)
    
    # ç§»é™¤ç‰¹æ®Šç¬¦å·å’Œæ— æ„ä¹‰çš„å†…å®¹
    unwanted_patterns = [
        r'é˜…è¯»é‡\d+[.\d]*[wkä¸‡åƒ]?',  # é˜…è¯»é‡ä¿¡æ¯
        r'æ”¶è—\d+[.\d]*[wkä¸‡åƒ]?',   # æ”¶è—ä¿¡æ¯
        r'ç‚¹èµ\d+[.\d]*[wkä¸‡åƒ]?',   # ç‚¹èµä¿¡æ¯
        r'å·²äº\s*\d{4}-\d{2}-\d{2}.*?ä¿®æ”¹',  # ä¿®æ”¹æ—¶é—´
        r'![^!]*![^!]*',  # è¿ç»­çš„æ„Ÿå¹å·å†…å®¹
        r'\s*åŸåˆ›\s*',  # åŸåˆ›æ ‡è®°
        r'\s*è½¬è½½\s*',  # è½¬è½½æ ‡è®°
        r'Â©.*?ç‰ˆæƒ.*?',  # ç‰ˆæƒä¿¡æ¯
        r'ç‰ˆæƒå£°æ˜.*?',  # ç‰ˆæƒå£°æ˜
        r'ç›¸å…³æ¨è.*?',  # ç›¸å…³æ¨è
        r'çŒœä½ å–œæ¬¢.*?',  # æ¨èå†…å®¹
        r'å¹¿å‘Š.*?',     # å¹¿å‘Šå†…å®¹
        r'ç™»å½•.*?æ³¨å†Œ', # ç™»å½•æ³¨å†Œæç¤º
        r'è®¢é˜….*?å…³æ³¨', # è®¢é˜…å…³æ³¨
        r'åˆ†äº«.*?è¯„è®º', # åˆ†äº«è¯„è®º
    ]
    
    for pattern in unwanted_patterns:
        content = re.sub(pattern, '', content, flags=re.IGNORECASE)
    
    # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
    content = re.sub(r'\n+', '\n', content)  # å¤šä¸ªæ¢è¡Œåˆå¹¶ä¸ºä¸€ä¸ª
    content = re.sub(r'\s+', ' ', content)   # å¤šä¸ªç©ºæ ¼åˆå¹¶ä¸ºä¸€ä¸ª
    content = re.sub(r'\s*\n\s*', ' ', content)  # æ¢è¡Œç¬¦å‘¨å›´çš„ç©ºæ ¼å¤„ç†
    
    # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ç‰¹æ®Šå­—ç¬¦
    content = re.sub(r'^[^\w\u4e00-\u9fff]*', '', content)  # ç§»é™¤å¼€å¤´éå­—æ¯æ•°å­—æ±‰å­—
    content = re.sub(r'[^\w\u4e00-\u9fff.!?]*$', '', content)  # ç§»é™¤ç»“å°¾ç‰¹æ®Šå­—ç¬¦
    
    # å»æ‰è¿‡çŸ­çš„ç‰‡æ®µï¼ˆå¯èƒ½æ˜¯æ— æ„ä¹‰å†…å®¹ï¼‰
    sentences = content.split('ã€‚')
    meaningful_sentences = []
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) > 10:  # åªä¿ç•™è¶³å¤Ÿé•¿çš„å¥å­
            meaningful_sentences.append(sentence)
    
    if meaningful_sentences:
        content = 'ã€‚'.join(meaningful_sentences)
        if not content.endswith('ã€‚') and not content.endswith('ï¼') and not content.endswith('ï¼Ÿ'):
            content += 'ã€‚'
    
    return content.strip()

def clean_abstract(abstract):
    """æ¸…ç†æ‘˜è¦æ–‡æœ¬"""
    if not abstract:
        return ""
    
    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    abstract = re.sub(r'\n+', ' ', abstract)
    abstract = re.sub(r'\s+', ' ', abstract)
    abstract = abstract.strip()
    
    # ç§»é™¤å¼€å¤´çš„"Abstract:"æ ‡è®° - ä½¿ç”¨æœ€ç®€å•çš„æ–¹æ³•
    # å…ˆè½¬ä¸ºå°å†™æ£€æŸ¥ï¼Œä½†ä¿æŒåŸæ–‡æœ¬è¿›è¡Œæ›¿æ¢
    abstract_lower = abstract.lower()
    if abstract_lower.startswith('abstract:'):
        abstract = abstract[9:].strip()  # ç§»é™¤"abstract:"
    elif abstract_lower.startswith('abstract '):
        abstract = abstract[9:].strip()  # ç§»é™¤"abstract "
    elif abstract_lower.startswith('abstract'):
        abstract = abstract[8:].strip()  # ç§»é™¤"abstract"
    
    # ç§»é™¤ä¸åº”è¯¥å‡ºç°åœ¨æ‘˜è¦ä¸­çš„å†…å®¹
    unwanted_patterns = [
        r'Download PDF.*?(?=\s|$)',
        r'Authors?:\s*.*?(?=\s|$)',
        r'Submitted on.*?(?=\s|$)',
        r'Comments:.*?(?=\s|$)',
        r'Subjects:.*?(?=\s|$)',
        r'\[.*?\]\(https?://.*?\)',  # markdowné“¾æ¥
        r'https?://[^\s]+',  # æ™®é€šé“¾æ¥
        r'arXiv:\d+\.\d+',  # arXiv ID
        r'^\s*\d+\s*$',  # çº¯æ•°å­—è¡Œ
    ]
    
    for pattern in unwanted_patterns:
        abstract = re.sub(pattern, '', abstract, flags=re.IGNORECASE)
    
    # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ç‰¹æ®Šå­—ç¬¦
    abstract = re.sub(r'^[^\w\s]*', '', abstract)
    abstract = re.sub(r'[^\w\s.!?]*$', '', abstract)
    
    return abstract.strip()

def is_valid_abstract(abstract):
    """æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ‘˜è¦"""
    if not abstract or len(abstract) < 50:
        return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸åº”è¯¥å‡ºç°åœ¨æ‘˜è¦ä¸­çš„å†…å®¹
    invalid_indicators = [
        r'(?i)download\s+pdf',
        r'(?i)authors?:\s*\[',
        r'(?i)submitted\s+on',
        r'(?i)comments:',
        r'(?i)subjects:',
        r'(?i)^\s*\d+\s*$',  # çº¯æ•°å­—
        r'(?i)^\s*table\s+of\s+contents',
        r'(?i)^\s*references\s*$',
    ]
    
    for pattern in invalid_indicators:
        if re.search(pattern, abstract):
            return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„å®é™…å†…å®¹ï¼ˆè‡³å°‘åŒ…å«ä¸€äº›å®Œæ•´çš„å¥å­ï¼‰
    sentences = re.split(r'[.!?]+', abstract)
    valid_sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
    
    return len(valid_sentences) >= 2

def is_valid_arxiv_url(url):
    """æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„arxiv.org/abs/æ ¼å¼URL"""
    if not url:
        return False
    # æ”¯æŒå¸¦ç‰ˆæœ¬å·çš„URLæ ¼å¼ï¼Œå¦‚ https://arxiv.org/abs/2402.01680v1
    return re.match(r'https://arxiv\.org/abs/\d+\.\d+(v\d+)?', url) is not None

if __name__ == "__main__":
    main()

