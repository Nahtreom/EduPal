# 20250730åŸrun_cosyvoice_batch.py

import os
import sys
import torch
import torchaudio
import argparse # ã€æ–°å¢ã€‘å¯¼å…¥å‚æ•°è§£ææ¨¡å—

# ã€æ–°å¢ã€‘è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æå™¨
parser = argparse.ArgumentParser(description="ä½¿ç”¨ CosyVoice åŠ¨æ€åˆæˆè¯­éŸ³")
parser.add_argument("input_dir", type=str, help="åŒ…å«å¾…åˆæˆ .txt æ–‡ä»¶çš„è¾“å…¥ç›®å½•è·¯å¾„")
parser.add_argument("output_dir", type=str, help="å­˜æ”¾åˆæˆå .wav æ–‡ä»¶çš„è¾“å‡ºç›®å½•è·¯å¾„")
parser.add_argument("--prompt_wav", type=str, default='./asset/zero_shot_prompt.wav', help="ç”¨äºéŸ³è‰²å…‹éš†çš„æç¤ºéŸ³é¢‘è·¯å¾„ (ä¾‹å¦‚ï¼Œç”¨æˆ·ä¸Šä¼ çš„éŸ³é¢‘)")
parser.add_argument("--prompt_text", type=str, default='å¸Œæœ›ä½ ä»¥åèƒ½å¤Ÿåšçš„æ¯”æˆ‘è¿˜å¥½å‘¦ã€‚', help="æç¤ºéŸ³é¢‘å¯¹åº”çš„æ–‡æœ¬ (ä¾‹å¦‚ï¼Œç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬)")
parser.add_argument("--voice_name", type=str, default='default', help="ç”¨äºåŒºåˆ†ä¸åŒéŸ³è‰²çš„åç§°ï¼Œæ–¹ä¾¿è°ƒè¯•å’Œç®¡ç†")

args = parser.parse_args() # è§£æä¼ å…¥çš„å‘½ä»¤è¡Œå‚æ•°

# --- åç»­ä»£ç ä½¿ç”¨è§£æåçš„å‚æ•° `args` ---

# è®¾ç½® CUDA è®¾å¤‡
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
sys.path.append('third_party/Matcha-TTS')

from cosyvoice.cli.cosyvoice import CosyVoice2
from cosyvoice.utils.file_utils import load_wav

# ä½¿ç”¨è§£æåçš„å‚æ•°
input_dir = args.input_dir
output_dir = args.output_dir
os.makedirs(output_dir, exist_ok=True)

# ... (summary_txt_filename çš„é€»è¾‘ä¿æŒä¸å˜) ...
input_dir_name = os.path.basename(os.path.normpath(input_dir))
summary_txt_filename = f"{input_dir_name}.txt"
summary_path = os.path.join(output_dir, summary_txt_filename)

# åŠ è½½ TTS æ¨¡å‹
print("æ­£åœ¨åŠ è½½ CosyVoice æ¨¡å‹...")
cosyvoice = CosyVoice2('pretrained_models/CosyVoice2-0.5B',
                       load_jit=False, load_trt=False, load_vllm=False, fp16=False)

# ã€ä¿®æ”¹ã€‘åŠ¨æ€åŠ è½½æç¤ºéŸ³
print(f"ğŸ”Š ä½¿ç”¨éŸ³è‰²: {args.voice_name}")
print(f"   - æç¤ºéŸ³æ–‡ä»¶: {args.prompt_wav}")
print(f"   - æç¤ºéŸ³æ–‡æœ¬: {args.prompt_text}")
prompt_speech_16k = load_wav(args.prompt_wav, 16000) # ä½¿ç”¨å‚æ•°æŒ‡å®šçš„è·¯å¾„
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ\n")

# ... (summary_lines, txt_files çš„é€»è¾‘ä¿æŒä¸å˜) ...
# è¾“å‡ºä¿¡æ¯è®°å½•
summary_lines = []

txt_files = sorted([f for f in os.listdir(input_dir) if f.endswith(".txt")])
total_files = len(txt_files)

# éå† txt æ–‡ä»¶å¹¶ç”Ÿæˆè¯­éŸ³
for idx, fname in enumerate(txt_files, 1):
    print(f"[{idx}/{total_files}] æ­£åœ¨å¤„ç†æ–‡ä»¶: {fname}")

    txt_path = os.path.join(input_dir, fname)
    with open(txt_path, "r", encoding="utf-8") as f:
        text = f.read().strip()

    segments = []
    print("å¼€å§‹åˆæˆè¯­éŸ³æ®µ...")
    for result in cosyvoice.inference_zero_shot(
        text,
        args.prompt_text, # ã€ä¿®æ”¹ã€‘ä½¿ç”¨å‚æ•°æŒ‡å®šçš„æç¤ºæ–‡æœ¬
        prompt_speech_16k,
        stream=False
    ):
        segments.append(result['tts_speech'])
    print(f"âœ… åˆæˆå®Œæˆï¼Œå…± {len(segments)} æ®µ")

    # ... (åç»­åˆå¹¶ã€ä¿å­˜ã€å†™å…¥summaryçš„é€»è¾‘ä¿æŒä¸å˜) ...
    if not segments:
        print("âš ï¸ æ— å¯ç”¨è¯­éŸ³æ®µï¼Œè·³è¿‡")
        continue

    combined_audio = torch.cat(segments, dim=1)
    output_wav_name = fname.replace(".txt", ".wav")
    output_wav_path = os.path.join(output_dir, output_wav_name)

    torchaudio.save(output_wav_path, combined_audio, cosyvoice.sample_rate)
    duration_sec = combined_audio.shape[1] / cosyvoice.sample_rate
    summary_lines.append(f"{output_wav_name}\t{duration_sec:.2f}s")
    print(f"âœ… åˆå¹¶å®Œæˆï¼Œå·²ä¿å­˜è‡³: {output_wav_path}ï¼ˆ{duration_sec:.2f}sï¼‰\n")

with open(summary_path, "w", encoding="utf-8") as f:
    f.write("\n".join(summary_lines))

print("âœ… æ‰€æœ‰è¯­éŸ³åˆæˆå®Œæˆï¼Œæ±‡æ€»ä¿¡æ¯å·²å†™å…¥:")
print(summary_path)